---
date: 2026-01-13T22:33:00
---
## 多模态

#### ViT（Vision Transformer，ICLR 2021）
#### 👀研究背景
1. Transformer在NLP邻域取得革命性成功，但是Transformer的全局注意力机制尚未在计算机视觉中成为主流；
2. 传统CNN依赖局部池化和卷积，建模**长距离依赖**能力有限，且存在**归纳偏置**的约束；
3. 此前尝试多为CNN-Transformer混合架构，未能充分发挥纯Transformer的全局建模能力；
#### 💡核心方法
![](assets/深度学习大杂烩/file-20260113224426174.png)
1. **图像序列化**：将图像分割为patch（16 $\times$ 16 或者 32 $\times$ 32），展平为序列token；
2. **嵌入层**：通过线性投影将patch转为向量，并添加class token（用于分类）与位置嵌入；
3. **Transformer编码器**：堆叠标准的Transformer块（多头注意力 + MLP），建立全局依赖；
4. **策略**：在JFT-300M等大规模数据集预训练，再在下游任务进行微调；

#### 🚀实验结果
- 🤖模型
- 
