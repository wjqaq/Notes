---
date: 2026-01-13T22:33:00
---

#### ViT（Vision Transformer，ICLR 2021）
##### 👀研究背景
1. Transformer在NLP邻域取得革命性成功，但是Transformer的全局注意力机制尚未在计算机视觉中成为主流；
2. 传统CNN依赖局部池化和卷积，建模**长距离依赖**能力有限，且存在**归纳偏置**的约束；
3. 此前尝试多为CNN-Transformer混合架构，未能充分发挥纯Transformer的全局建模能力。
##### 💡核心方法
![](assets/多模态/file-20260114001716508.png)
1. **图像序列化**：将图像分割为patch（16 $\times$ 16 或者 32 $\times$ 32），展平为序列token；
2. **嵌入层**：通过线性投影将patch转为向量，并添加class token（用于分类）与位置嵌入；
3. **Transformer编码器**：堆叠标准的Transformer块（多头注意力 + MLP），建立全局依赖；
4. **策略**：在JFT-300M等大规模数据集预训练，再在下游任务进行微调。

##### 🚀实验结果
- 🤖模型
	- 🧱ViT-Base；
	- 🪨ViT-Large；
	- ⛰️ViT-Huge。
- 📊结果![](assets/多模态/file-20260114001716511.png)
  在视觉任务上达到了SOTA
##### 📈影响
- 首次证明纯 Transformer 无需 CNN 的归纳偏置，在视觉任务上也能达到 SOTA；
- 为后续 CLIP、BLIP 等多模态模型提供了统一的视觉输入格式。

#### CLIP（Contrastive Language-Image Pre-training，ICML 2021）
##### 👀研究背景
- 传统视觉模型依赖人工标注类别标签，泛化性受限，难以处理未定义类别；
- 自然语言蕴含丰富语义监督，可作为更灵活的学习信号；
- 现有图文方法多为细粒度预测（如标题生成），缺乏面向**跨任务迁移**的通用表征学习方案。
##### 💡核心方法
![](assets/多模态/file-20260114001716511%201.png)
1. **双编码架构**：
	- 🧩图像编码器：ViT或CNN，输出图像特征向量；
	- 📃文本编码器：Transformer，输出文本特征向量。
2. **对比学习目标**：
	- 构建 N×N 对比矩阵，优化使匹配图文对相似度高于不匹配对；
	- 预训练数据：4 亿高质量图像 - 文本对（Web 收集并过滤）。
![](assets/多模态/file-20260114001716513.png)
3. **零样本推理**：
	- 将分类任务转为图文匹配：用类别名称构造文本描述，取最高相似度类别。
##### 🚀实验结果
![](assets/多模态/file-20260114001716513%201.png)
- 在27个数据集的评估套件中，零样本CLIP分类器在16个数据集（包括ImageNet）上优于在ResNet-50特征上拟合的全监督线性分类器；
- 开创 **提示工程（Prompt Engineering** 在视觉领域的应用，推动多模态融合方向。
##### 📈影响
- **监督方式革新**：用 “自然语言监督” 替代传统人工标注，大幅降低了视觉模型的训练成本，开启了 “以文训图” 的新方向；
- **零样本能力突破**：首次实现视觉模型的通用零样本迁移，让模型能够识别训练集中未出现的类别，推动了通用视觉模型的发展。